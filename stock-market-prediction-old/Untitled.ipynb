{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "f = codecs.open('news.txt', 'r', 'utf-8')\n",
    "news = [eval(i) for i in f.readlines()]\n",
    "f.close()\n",
    "print(news[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(news)):\n",
    "    a = news[i]\n",
    "    content = a['content']\n",
    "    title = a['title']\n",
    "    id = a['id']\n",
    "    if id in [84699,4871183,84702]:\n",
    "        print(id,title)\n",
    "        print('------')\n",
    "        print(content)\n",
    "        print('------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('train.txt', 'r')\n",
    "train = [i.split() for i in f.readlines()]\n",
    "f.close\n",
    "f = open('test.txt', 'r')\n",
    "test = [i.split() for i in f.readlines()]\n",
    "f.close\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a ='+1 +2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "s = '我爱:是！'\n",
    "r1 = '[a-zA-Z0-9’!\"#$%&\\'()*+,-./:;<=>?@，。?★、…【】《》？“”‘’！[\\$$^_`{|}~]+'\n",
    "s = re.sub(r1, '', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f = open('stopwords.txt', mode='r', encoding='utf-8')\n",
    "stopWords = [i for i in f.read() if i !='\\n']\n",
    "f.close()\n",
    "print(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "s = '央企合并六大传闻靠谱度排行：谁是下一个中国神车'\n",
    "cut = jieba.cut(s)\n",
    "result = ' '.join(cut)\n",
    "result = result.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import jieba\n",
    "\n",
    "\n",
    "def train_sample(train, news):\n",
    "    for i in range(len(train)):\n",
    "        id_str = train[i][1].split(',')\n",
    "        id_list = [int(i) for i in id_str]\n",
    "        title_list = []\n",
    "        content_list = []\n",
    "        for j in range(len(news)):\n",
    "            a = news[j]\n",
    "            content = a['content']\n",
    "            title = a['title']\n",
    "            id = a['id']\n",
    "            if id in id_list:\n",
    "                title_list.append(title)\n",
    "                content_list.append(content)\n",
    "        train[i].append(title_list)\n",
    "        train[i].append(content_list)\n",
    "        instance = train[i]\n",
    "        train[i] = (features(instance[3]),instance[0])\n",
    "    return train\n",
    "\n",
    "\n",
    "def test_sample(test, news):\n",
    "    test_copy = test.copy()\n",
    "    for i in range(len(test_copy)):\n",
    "        id_str = test_copy[i][1].split(',')\n",
    "        id_list = [int(i) for i in id_str]\n",
    "        title_list = []\n",
    "        content_list = []\n",
    "        for j in range(len(news)):\n",
    "            a = news[j]\n",
    "            content = a['content']\n",
    "            title = a['title']\n",
    "            id = a['id']\n",
    "            if id in id_list:\n",
    "                title_list.append(title)\n",
    "                content_list.append(content)\n",
    "        test_copy[i].append(title_list)\n",
    "        test_copy[i].append(content_list)\n",
    "        instance = test_copy[i]\n",
    "        print(type(instance),len(instance))\n",
    "        print('instance:',instance)\n",
    "        print(instance[0])\n",
    "        print(instance[1])\n",
    "        print(instance[2])\n",
    "        test_copy[i] = (features(instance[2]),instance[0])\n",
    "        break\n",
    "    return test_copy\n",
    "test_sample(test,news)\n",
    "\n",
    "def features(words): #特征提取器\n",
    "\n",
    "    f = open('stopwords.txt', mode='r', encoding='utf-8')\n",
    "    stopWords = [i for i in f.read() if i != '\\n']\n",
    "    f.close()\n",
    "\n",
    "    result = []\n",
    "    for i in range(len(words)):\n",
    "        s = words[i]\n",
    "        cut = jieba.cut(s)\n",
    "        cut = ' '.join(cut)\n",
    "        cut = cut.split()\n",
    "        temp = []\n",
    "        for word in cut:\n",
    "            if word not in stopWords:\n",
    "                temp.append(word)\n",
    "        result = result + temp\n",
    "        break\n",
    "    return dict([(word, True) for word in result])\n",
    "\n",
    "\n",
    "f = open('news.txt', mode='r', encoding='utf-8')\n",
    "news = [eval(i) for i in f.readlines()]\n",
    "f.close()\n",
    "\n",
    "f = open('train.txt', mode='r', encoding='utf-8')\n",
    "train = [i.split() for i in f.readlines()]\n",
    "f.close\n",
    "\n",
    "f = open('test.txt', mode='r', encoding='utf-8')\n",
    "test = [i.split() for i in f.readlines()]\n",
    "f.close\n",
    "'''\n",
    "train = train_sample(train, news)\n",
    "print('--------------------------------------------------------')\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train) #生成分类器\n",
    "\n",
    "test1 = test_sample(test, news)\n",
    "print(test1[0])\n",
    "\n",
    "f = open('result.txt', 'w')\n",
    "for i in range(len(test1)):\n",
    "    tag = classifier.classify(test1[i][0])#分类\n",
    "    f.write(tag+' '+test[i][1]+'\\n')\n",
    "f.close()\n",
    "\n",
    "\n",
    "#print(nltk.classify.accuracy(classifier,test)) #测试准确度\n",
    "\n",
    "#classifier.show_most_informative_features(5)#得到似然比，检测对于哪些特征有用\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('news.txt', mode='r', encoding='utf-8')\n",
    "news = [eval(i) for i in f.readlines()]\n",
    "f.close()\n",
    "\n",
    "f = open('train.txt', mode='r', encoding='utf-8')\n",
    "train = [i.split() for i in f.readlines()]\n",
    "f.close\n",
    "\n",
    "f = open('test.txt', mode='r', encoding='utf-8')\n",
    "test = [i.split() for i in f.readlines()]\n",
    "f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(type(test))\n",
    "news[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_sample(test, news):\n",
    "    test_copy = test.copy()\n",
    "    for i in range(len(test_copy)):\n",
    "        id_str = test_copy[i][1].split(',')\n",
    "        id_list = [int(i) for i in id_str]\n",
    "        title_list = []\n",
    "        content_list = []\n",
    "        for j in range(len(news)):\n",
    "            a = news[j]\n",
    "            content = a['content']\n",
    "            title = a['title']\n",
    "            id = a['id']\n",
    "            if id in id_list:\n",
    "                title_list.append(title)\n",
    "                content_list.append(content)\n",
    "        test_copy[i].append(title_list)\n",
    "        test_copy[i].append(content_list)\n",
    "        instance = test_copy[i]\n",
    "        print(type(instance),len(instance))\n",
    "        print('instance:',instance)\n",
    "        print(instance[0])\n",
    "        print(instance[1])\n",
    "        print(instance[2])\n",
    "        print(instance[3])\n",
    "        test_copy[i] = (features(instance[2]),instance[0])\n",
    "        break\n",
    "    return test_copy\n",
    "test_sample(test.copy(),news)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-919b27c0b6b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1379\u001b[0m             \u001b[0mTf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0midf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1380\u001b[0m         \"\"\"\n\u001b[1;32m-> 1381\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1382\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1383\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 869\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    870\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 792\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    793\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 266\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X_test = [u'没有 你 的 地方 都是 他乡',u'弯弯 的 月亮',u'没有 你 的 旅行 都是 流浪']\n",
    "\n",
    "tfidf=TfidfVectorizer()\n",
    "weight=tfidf.fit_transform(X_test).toarray()\n",
    "word=tfidf.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------这里输出第 0 类文本的词语tf-idf权重------\n",
      "他乡 0.5628290964997665\n",
      "你喜欢吃什么东西 0.0\n",
      "地方 0.5628290964997665\n",
      "旅行 0.0\n",
      "没有 0.4280460350631185\n",
      "流浪 0.0\n",
      "都是 0.4280460350631185\n",
      "-------这里输出第 1 类文本的词语tf-idf权重------\n",
      "他乡 0.0\n",
      "你喜欢吃什么东西 1.0\n",
      "地方 0.0\n",
      "旅行 0.0\n",
      "没有 0.0\n",
      "流浪 0.0\n",
      "都是 0.0\n",
      "-------这里输出第 2 类文本的词语tf-idf权重------\n",
      "他乡 0.0\n",
      "你喜欢吃什么东西 0.0\n",
      "地方 0.0\n",
      "旅行 0.5628290964997665\n",
      "没有 0.4280460350631185\n",
      "流浪 0.5628290964997665\n",
      "都是 0.4280460350631185\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(weight)):  \n",
    "# 打印每类文本的tf-idf词语权重，第一个for遍历所有文本，\n",
    "#第二个for便利某一类文本下的词语权重\n",
    "    print(u\"-------这里输出第\", i, u\"类文本的词语tf-idf权重------\")\n",
    "    for j in range(len(word)):\n",
    "        print(word[j], weight[i][j])#第i个文本中，第j个次的tfidf值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "import numpy as np\n",
    "\n",
    "nums = [1, 8, 2, 23, 7, -4]\n",
    "\n",
    "# 最大的3个数的索引\n",
    "max_num_index_list = map(nums.index, heapq.nlargest(3, nums))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = list(map(nums.index, heapq.nlargest(3, nums)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-f2a0d1c8939b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnums\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "nums(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 8, 2, 23, 7, -4]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(a[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
